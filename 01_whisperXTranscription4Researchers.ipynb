{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.0 WhisperX/Pyannote Transcription+Diarization Pipeline \n",
    "\n",
    "This Jupyter notebook is designed to test and evaluate a new Transcription and Diarization Pipeline with the following objectives:\n",
    "1. Achieving word-level transcription accuracy to ensure detailed and precise text representation of the audio input.\n",
    "2. Assessing diarization confidence levels to accurately attribute spoken segments to different speakers and measure the reliability of speaker identification.\n",
    "3. Enhancing the alignment of transcriptions to be closer to natural sentence segments, thereby improving the readability and usability of the transcribed data.\n",
    "\n",
    "The notebook leverages advanced transcription and diarization capabilities provided by the Whisper, WhisperX, and pyannote libraries. By using GPU acceleration, it processes audio data efficiently, performing alignment and diarization to produce structured outputs that are saved in CSV format for further analysis. The resources and installation instructions are included to facilitate the setup and execution of the pipeline.\n",
    "\n",
    "Resources:\n",
    "https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Setup\n",
    "WhisperX documentation found here: https://github.com/m-bain/whisperX\n",
    "\n",
    "1. Create Python environment\n",
    "conda create -n whisperx-env python=3.9\n",
    "conda activate whisperx-env\n",
    "\n",
    "2. Install PyTorch https://pytorch.org/get-started/locally/ \n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "3. Install WhisperX repository\n",
    "pip install git+https://github.com/m-bain/whisperx.git\n",
    "\n",
    "4. Additional useful packages to install\n",
    "pip install charset-normalizer\n",
    "pip install pandas\n",
    "pip install nltk\n",
    "pip install numpy\n",
    "pip install plotly\n",
    "pip install matplotlib\n",
    "pip install jupyter ipywidgets\n",
    "pip install webvtt-py\n",
    "pip install pypi-json\n",
    "pip install srt\n",
    "\n",
    "5. Create .env file at the same level as this notebook file with the following line\n",
    "HF_TOKEN=\"REPLACEWITHHUGGINGFACETOKENHERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Check once to see if CUDA GPU is available and PyTorch is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Laptop GPU')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA GPU is available to PyTorch\n",
    "import torch                                                # PyTorch\n",
    "torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()      # Check if GPU is available and get the name of the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Start here by adjusting variables\n",
    "1. choose batch size, compute type, whisper model, and file extension to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configuration\n",
    "torch.cuda.set_device(0)  # Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "whisperx_model = \"small.en\"\n",
    "extensions = ['.ogg', '.m4a', '.mp3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Run after adjusting variables first\n",
    "\n",
    "Just push run here. You shouldn't need to change anything here unless you want to output less or more file types. These are mostly functions which are then called at the end of the cell.\n",
    "\n",
    "1. You should get a popup asking to choose the folder where the files are found (It will also search subfolders).\n",
    "\n",
    "2. You should then get a popup asking for where the transcription files should be placed (It will replicate the folder structure in which they were found)\n",
    "\n",
    "3. You will also see a popup asking if you want to anonymize with a pseudonyms.csv file, and if so where it is located.\n",
    "\n",
    "4. You should then see an output similar to the following (just ignore the warnings):\n",
    "\n",
    "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
    "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
    "\n",
    "5. When complete you will see where each were written and the folders where they were written to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = str(datetime.timedelta(seconds=segment['start']))\n",
    "        caption.end = str(datetime.timedelta(seconds=segment['end']))\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Bring the root window to the front\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file for to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files to process.\n",
      "Processing D:/NLPWork/WhisperXTranscription4Researchers/data/rawAudioFiles\\monos\\Mono1\\Monologue1.ogg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\mrhal\\anaconda3\\envs\\whisperX-env\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Processed D:/NLPWork/WhisperXTranscription4Researchers/data/rawAudioFiles\\monos\\Mono1\\Monologue1.ogg\n",
      "Processing D:/NLPWork/WhisperXTranscription4Researchers/data/rawAudioFiles\\monos\\Mono2\\Monologue2.ogg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\mrhal\\anaconda3\\envs\\whisperX-env\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Processed D:/NLPWork/WhisperXTranscription4Researchers/data/rawAudioFiles\\monos\\Mono2\\Monologue2.ogg\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog, messagebox\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configuration\n",
    "torch.cuda.set_device(0)  # Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "whisperx_model = \"small.en\"\n",
    "extensions = ['.ogg', '.m4a', '.mp3']\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = str(datetime.timedelta(seconds=segment['start']))\n",
    "        caption.end = str(datetime.timedelta(seconds=segment['end']))\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperX-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
