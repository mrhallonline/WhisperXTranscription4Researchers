{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.0 WhisperX/Pyannote Transcription+Diarization Pipeline \n",
    "\n",
    "This Jupyter notebook is designed to test and evaluate a new Transcription and Diarization Pipeline with the following objectives:\n",
    "1. Achieving word-level transcription accuracy to ensure detailed and precise text representation of the audio input.\n",
    "2. Assessing diarization confidence levels to accurately attribute spoken segments to different speakers and measure the reliability of speaker identification.\n",
    "3. Enhancing the alignment of transcriptions to be closer to natural sentence segments, thereby improving the readability and usability of the transcribed data.\n",
    "\n",
    "The notebook leverages advanced transcription and diarization capabilities provided by the Whisper, WhisperX, and pyannote libraries. By using GPU acceleration, it processes audio data efficiently, performing alignment and diarization to produce structured outputs that are saved in CSV format for further analysis. The resources and installation instructions are included to facilitate the setup and execution of the pipeline.\n",
    "\n",
    "Resources:\n",
    "https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Setup\n",
    "WhisperX documentation found here: https://github.com/m-bain/whisperX\n",
    "================================================\n",
    "1. Install Git\n",
    "2. Install FFMPEG and add to PATH\n",
    "3. Install Anaconda \n",
    "\n",
    "================================================   \n",
    "4. Create Conda environment\n",
    "conda create -n whisperxtranscription-env python=3.10\n",
    "conda activate whisperxtranscription-env\n",
    "\n",
    "5. Install PyTorch https://pytorch.org/get-started/locally/ \n",
    "pip install numpy==1.26.3 torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "6. Install WhisperX repository and additional packages\n",
    "pip install whisperx speechbrain jupyter ipywidgets charset-normalizer pandas nltk plotly matplotlib webvtt-py pypi-json srt python-dotenv\n",
    "\n",
    "7. Create .env file at the same level as this notebook file with the following line\n",
    "HF_TOKEN=\"REPLACEWITHHUGGINGFACETOKENHERE\"\n",
    "\n",
    "=================================================\n",
    "8. For GPU usage :\n",
    "Install Visual Studio Community https://visualstudio.microsoft.com/downloads/\n",
    "Install NVIDIA CUDA Toolkit 12.1 https://developer.nvidia.com/cuda-12-1-0-download-archive \n",
    "\n",
    "Check PyTorch and CUDA installation\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "=================================================\n",
    "Fix Numpy\n",
    "pip uninstall numpy -y\n",
    "pip install numpy==1.26.3\n",
    "\n",
    "Fix PyTorch\n",
    "pip uninstall torch torchvision torchaudio -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.3\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/m-bain/whisperx.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Check once to see if CUDA GPU is available and PyTorch is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 3090')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA GPU is available to PyTorch\n",
    "import torch                                                # PyTorch\n",
    "torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()      # Check if GPU is available and get the name of the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Start here by adjusting variables\n",
    "1. choose batch size, compute type, whisper model, and file extension to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configuration\n",
    "torch.cuda.set_device(0)  # Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "whisperx_model = \"large-v3-turbo\"\n",
    "extensions = ['.ogg', '.m4a', '.mp3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Run after adjusting variables first\n",
    "\n",
    "Just push run here. You shouldn't need to change anything here unless you want to output less or more file types. These are mostly functions which are then called at the end of the cell.\n",
    "\n",
    "1. You should get a popup asking to choose the folder where the files are found (It will also search subfolders).\n",
    "\n",
    "2. You should then get a popup asking for where the transcription files should be placed (It will replicate the folder structure in which they were found)\n",
    "\n",
    "3. You will also see a popup asking if you want to anonymize with a pseudonyms.csv file, and if so where it is located.\n",
    "\n",
    "4. You should then see an output similar to the following (just ignore the warnings):\n",
    "\n",
    "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
    "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
    "\n",
    "5. When complete you will see where each were written and the folders where they were written to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files to process.\n",
      "Processing E:/My Documents/AANLPWork/WhisperXTranscription4Researchers/Data/rawAudioFiles\\Monologue1.ogg...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22133b3c50f64ab08e7ad2f89b4ed0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ba2ff32886416885dda0b1021a0093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6334c599b5c04edcb54b79c98aca6ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.json:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99585ac3398a4aaba3ff62c127f78bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51e221e613247679dcb994e1e770a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint e:\\anaconda3\\envs\\whisperxtranscription-env\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (1.00) in first 30s of audio...\n",
      "Processed E:/My Documents/AANLPWork/WhisperXTranscription4Researchers/Data/rawAudioFiles\\Monologue1.ogg\n",
      "Processing E:/My Documents/AANLPWork/WhisperXTranscription4Researchers/Data/rawAudioFiles\\Monologue2.ogg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint e:\\anaconda3\\envs\\whisperxtranscription-env\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (1.00) in first 30s of audio...\n",
      "Processed E:/My Documents/AANLPWork/WhisperXTranscription4Researchers/Data/rawAudioFiles\\Monologue2.ogg\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def format_vtt_timestamp(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = format_vtt_timestamp(segment['start'])\n",
    "        caption.end = format_vtt_timestamp(segment['end'])\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Bring the root window to the front\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file for to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperxtranscription-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
