{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.0 WhisperX/Pyannote Transcription+Diarization Pipeline \n",
    "\n",
    "This Jupyter notebook is designed to test and evaluate a new Transcription and Diarization Pipeline with the following objectives:\n",
    "1. Achieving word-level transcription accuracy to ensure detailed and precise text representation of the audio input.\n",
    "2. Assessing diarization confidence levels to accurately attribute spoken segments to different speakers and measure the reliability of speaker identification.\n",
    "3. Enhancing the alignment of transcriptions to be closer to natural sentence segments, thereby improving the readability and usability of the transcribed data.\n",
    "\n",
    "The notebook leverages advanced transcription and diarization capabilities provided by the Whisper, WhisperX, and pyannote libraries. By using GPU acceleration, it processes audio data efficiently, performing alignment and diarization to produce structured outputs that are saved in CSV format for further analysis. The resources and installation instructions are included to facilitate the setup and execution of the pipeline.\n",
    "\n",
    "Resources:\n",
    "https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Setup\n",
    "WhisperX documentation found here: https://github.com/m-bain/whisperX\n",
    "================================================\n",
    "1. Install Git\n",
    "2. Install FFMPEG and add to PATH\n",
    "3. Install Anaconda \n",
    "\n",
    "================================================   \n",
    "4. Create Conda environment\n",
    "conda create -n whisperxtranscription-env python=3.10\n",
    "conda activate whisperxtranscription-env\n",
    "\n",
    "5. Install PyTorch https://pytorch.org/get-started/locally/ \n",
    "pip install numpy==1.26.3 torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "6. Install WhisperX repository and additional packages\n",
    "pip install whisperx speechbrain jupyter ipywidgets charset-normalizer pandas nltk plotly matplotlib webvtt-py pypi-json srt python-dotenv\n",
    "\n",
    "7. Create .env file at the same level as this notebook file with the following line\n",
    "HF_TOKEN=\"REPLACEWITHHUGGINGFACETOKENHERE\"\n",
    "\n",
    "=================================================\n",
    "8. For GPU usage :\n",
    "Install Visual Studio Community https://visualstudio.microsoft.com/downloads/\n",
    "Install NVIDIA CUDA Toolkit 12.1 https://developer.nvidia.com/cuda-12-1-0-download-archive \n",
    "\n",
    "Check PyTorch and CUDA installation\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "=================================================\n",
    "Fix Numpy\n",
    "pip uninstall numpy -y\n",
    "pip install numpy==1.26.3\n",
    "\n",
    "Fix PyTorch\n",
    "pip uninstall torch torchvision torchaudio -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.3\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/m-bain/whisperx.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Check once to see if CUDA GPU is available and PyTorch is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Laptop GPU')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA GPU is available to PyTorch\n",
    "import torch                                                # PyTorch\n",
    "torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()      # Check if GPU is available and get the name of the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Setup - Start here by adjusting variables\n",
    "1. choose batch size, compute type, whisper model, and file extension to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configuration\n",
    "torch.cuda.set_device(0)  # Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Set device to GPU if available, otherwise CPU\n",
    "language = \"en\"  # Set the language code en=English, es=Spanish, etc.\n",
    "task = \"transcribe\"  # Set the task to \"transcribe\" or \"translate\" \n",
    "batch_size = 16 # Set the batch size for processing\n",
    "compute_type = \"float16\" # Set the compute type to \"float16\" for faster processing\n",
    "hf_token = os.getenv('HF_TOKEN') \n",
    "whisperx_model = \"large-v3-turbo\" # Set the WhisperX model to use\n",
    "extensions = ['.WAV', '.m4a', '.mp3'] # Supported audio file extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Run - after adjusting variables first\n",
    "\n",
    "Just push run here. You shouldn't need to change anything here unless you want to output less or more file types. These are mostly functions which are then called at the end of the cell.\n",
    "\n",
    "1. You should get a popup asking to choose the folder where the files are found (It will also search subfolders).\n",
    "\n",
    "2. You should then get a popup asking for where the transcription files should be placed (It will replicate the folder structure in which they were found)\n",
    "\n",
    "3. You will also see a popup asking if you want to anonymize with a pseudonyms.csv file, and if so where it is located.\n",
    "\n",
    "4. You should then see an output similar to the following (just ignore the warnings):\n",
    "\n",
    "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
    "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
    "\n",
    "5. When complete you will see where each were written and the folders where they were written to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No folder selected. Exiting.\n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def format_vtt_timestamp(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = format_vtt_timestamp(segment['start'])\n",
    "        caption.end = format_vtt_timestamp(segment['end'])\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size, language=language, task=task)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Bring the root window to the front\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file for to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to process.\n",
      "Processing C:/Users/mrhal/OneDrive/Desktop/Spanish_Interview/Group_3_Guadalupe_and_Araceli\\Group_3_Guadalupe_and_Araceli-Audio.WAV...\n",
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\mrhal\\anaconda3\\envs\\whisperX-env\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Processed C:/Users/mrhal/OneDrive/Desktop/Spanish_Interview/Group_3_Guadalupe_and_Araceli\\Group_3_Guadalupe_and_Araceli-Audio.WAV\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import datetime\n",
    "import json\n",
    "import webvtt\n",
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configuration\n",
    "torch.cuda.set_device(0)  # Set GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "language = \"en\"  # Set the language code en=English, es=Spanish, etc.\n",
    "task = \"transcribe\"  # Set the task to \"translate\"\n",
    "batch_size = 16\n",
    "compute_type = \"float16\"\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "whisperx_model = \"large-v3-turbo\"\n",
    "extensions = ['.WAV', '.m4a', '.mp3']\n",
    "\n",
    "\n",
    "# Functions\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def format_vtt_timestamp(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    if pseudonym_dict:\n",
    "        for segment in segments:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    df = pd.DataFrame(segments)\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}_transcription.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = format_vtt_timestamp(segment['start'])\n",
    "        caption.end = format_vtt_timestamp(segment['end'])\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt.save(os.path.join(output_dir, f\"{base_filename}_transcription.vtt\"))\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    try:\n",
    "        print(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size, language=language, task=task)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Tkinter\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Bring the root window to the front\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    # Popup for input folder\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Popup for output folder\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        print(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Ask if a pseudonyms.csv file will be used\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file for to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            print(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            # Load the pseudonyms file\n",
    "            pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "            pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "            print(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "\n",
    "    # Find and process audio files\n",
    "    audio_files = find_audio_files(input_folder, extensions)\n",
    "    print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "        print(f\"Processed {audio_file}\")\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempted Gemini Revision Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 17:59:43,633 - INFO - Found 1 files to process.\n",
      "Processing Audio Files:   0%|          | 0/1 [00:00<?, ?it/s]2025-04-22 17:59:43,635 - INFO - Processing C:/Users/mrhal/OneDrive/Desktop/Spanish_Interview/Group_3_Guadalupe_and_Araceli\\Group_3_Guadalupe_and_Araceli-Audio.WAV...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\mrhal\\anaconda3\\envs\\whisperX-env\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Failed to align segment (\" That's all for now.\"): backtrack failed, resorting to original...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 18:00:56,075 - INFO - Saved CSV transcription to C:/Users/mrhal/OneDrive/Desktop/New folder\\Group_3_Guadalupe_and_Araceli-Audio_transcription.csv\n",
      "2025-04-22 18:00:56,076 - INFO - Saved TXT transcription to C:/Users/mrhal/OneDrive/Desktop/New folder\\Group_3_Guadalupe_and_Araceli-Audio_transcription.txt\n",
      "2025-04-22 18:00:56,089 - INFO - Saved JSON transcription to C:/Users/mrhal/OneDrive/Desktop/New folder\\Group_3_Guadalupe_and_Araceli-Audio_transcription.json\n",
      "2025-04-22 18:00:56,094 - INFO - Saved VTT transcription to C:/Users/mrhal/OneDrive/Desktop/New folder\\Group_3_Guadalupe_and_Araceli-Audio_transcription.vtt\n",
      "2025-04-22 18:00:56,095 - INFO - Finished processing C:/Users/mrhal/OneDrive/Desktop/Spanish_Interview/Group_3_Guadalupe_and_Araceli\\Group_3_Guadalupe_and_Araceli-Audio.WAV\n",
      "Processing Audio Files: 100%|██████████| 1/1 [01:12<00:00, 72.47s/it]\n",
      "2025-04-22 18:00:56,103 - INFO - All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog, messagebox\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "import json\n",
    "import webvtt\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import configparser  # Example for config file\n",
    "\n",
    "# --- Configuration (using configparser as an example) ---\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "DEVICE = config['whisper']['device']\n",
    "LANGUAGE = config['whisper']['language']\n",
    "WHISPERX_MODEL = config['whisper']['model']\n",
    "BATCH_SIZE = config['processing'].getint('batch_size')\n",
    "COMPUTE_TYPE = config['processing']['compute_type']\n",
    "EXTENSIONS_STR = config['files']['extensions']\n",
    "EXTENSIONS = [ext.strip() for ext in EXTENSIONS_STR.split(',')]\n",
    "HF_TOKEN = config['auth']['hf_token']\n",
    "TASK = config['whisper']['task']\n",
    "CHUNK_SIZE=30\n",
    "TEMPERATURE=0.0\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Functions ---\n",
    "def find_audio_files(base_dir, extensions):\n",
    "    \"\"\"Finds all audio files with specified extensions in a directory and its subdirectories.\"\"\"\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    \"\"\"Replaces real names in text with pseudonyms from a dictionary.\"\"\"\n",
    "    if pseudonym_dict:\n",
    "        for real_name, pseudonym in pseudonym_dict.items():\n",
    "            text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "def format_vtt_timestamp(seconds):\n",
    "    \"\"\"Formats a time in seconds to the WebVTT timestamp format (HH:MM:SS.mmm).\"\"\"\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
    "\n",
    "def save_transcripts(segments, output_dir, relative_path, pseudonym_dict=None):\n",
    "    \"\"\"Saves the transcription segments to CSV, TXT, JSON, and VTT files.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    base_filename = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "\n",
    "    # Anonymize text if a pseudonym dictionary is provided\n",
    "    processed_segments = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['text'] = segment['text'].lstrip()\n",
    "        if pseudonym_dict:\n",
    "            segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "        segment['sentence_number'] = i + 1\n",
    "        processed_segments.append(segment.copy())  # Avoid modifying original list\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(processed_segments)\n",
    "    cols = ['sentence_number'] + [col for col in df.columns if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "    csv_path = os.path.join(output_dir, f\"{base_filename}_transcription.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    logging.info(f\"Saved CSV transcription to {csv_path}\")\n",
    "\n",
    "    # Save to TXT\n",
    "    txt_path = os.path.join(output_dir, f\"{base_filename}_transcription.txt\")\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        for segment in processed_segments:\n",
    "            f.write(f\"{segment['text'].strip()}\\n\")\n",
    "    logging.info(f\"Saved TXT transcription to {txt_path}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    json_path = os.path.join(output_dir, f\"{base_filename}_transcription.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_segments, f, ensure_ascii=False, indent=4)\n",
    "    logging.info(f\"Saved JSON transcription to {json_path}\")\n",
    "\n",
    "    # Save to VTT\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in processed_segments:\n",
    "        caption = webvtt.Caption()\n",
    "        caption.start = format_vtt_timestamp(segment['start'])\n",
    "        caption.end = format_vtt_timestamp(segment['end'])\n",
    "        caption.lines = [f\"{segment['sentence_number']}: {segment['text'].strip()}\"]\n",
    "        vtt.captions.append(caption)\n",
    "    vtt_path = os.path.join(output_dir, f\"{base_filename}_transcription.vtt\")\n",
    "    vtt.save(vtt_path)\n",
    "    logging.info(f\"Saved VTT transcription to {vtt_path}\")\n",
    "\n",
    "def process_audio_file(audio_file, base_output_dir, relative_path, pseudonym_dict=None):\n",
    "    \"\"\"Processes a single audio file: transcribes, aligns, diarizes, and saves the output.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Processing {audio_file}...\")\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        model = whisperx.load_model(WHISPERX_MODEL, DEVICE, compute_type=COMPUTE_TYPE)\n",
    "        result = model.transcribe(audio, batch_size=BATCH_SIZE, language=LANGUAGE, task=TASK, chunk_size=CHUNK_SIZE, temperature=TEMPERATURE)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=LANGUAGE, device=DEVICE)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, DEVICE, return_char_alignments=False)\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=DEVICE)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "        output_dir = os.path.join(base_output_dir, os.path.dirname(relative_path))\n",
    "        save_transcripts(result[\"segments\"], output_dir, relative_path, pseudonym_dict)\n",
    "        logging.info(f\"Finished processing {audio_file}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Audio file not found: {audio_file}\")\n",
    "    except RuntimeError as e:\n",
    "        logging.error(f\"CUDA error during processing of {audio_file}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while processing {audio_file}: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to handle user input and process audio files.\"\"\"\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    input_folder = filedialog.askdirectory(title=\"Select Folder Containing Audio/Video Files\")\n",
    "    if not input_folder:\n",
    "        logging.info(\"No input folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    output_folder = filedialog.askdirectory(title=\"Select Folder to Save Transcriptions\")\n",
    "    if not output_folder:\n",
    "        logging.info(\"No output folder selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    use_pseudonyms = messagebox.askyesno(\"Pseudonyms\", \"Will you use a pseudonyms.csv file to anonymize the transcripts?\")\n",
    "    pseudonym_dict = None\n",
    "\n",
    "    if use_pseudonyms:\n",
    "        pseudonyms_file = filedialog.askopenfilename(\n",
    "            title=\"Select Pseudonyms CSV File\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "        )\n",
    "        if not pseudonyms_file:\n",
    "            logging.info(\"No pseudonyms file selected. Continuing without pseudonymization.\")\n",
    "        else:\n",
    "            try:\n",
    "                pseudonyms_df = pd.read_csv(pseudonyms_file)\n",
    "                pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))\n",
    "                logging.info(f\"Pseudonyms loaded from {pseudonyms_file}.\")\n",
    "            except FileNotFoundError:\n",
    "                messagebox.showerror(\"Error\", f\"Pseudonyms file not found: {pseudonyms_file}\")\n",
    "                logging.error(f\"Pseudonyms file not found: {pseudonyms_file}\")\n",
    "            except KeyError:\n",
    "                messagebox.showerror(\"Error\", \"Pseudonyms CSV file must have 'name' and 'pseudonym' columns.\")\n",
    "                logging.error(\"Pseudonyms CSV file must have 'name' and 'pseudonym' columns.\")\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"Error reading pseudonyms file: {e}\")\n",
    "                logging.error(f\"Error reading pseudonyms file: {e}\")\n",
    "\n",
    "    audio_files = find_audio_files(input_folder, EXTENSIONS)\n",
    "    logging.info(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing Audio Files\"):\n",
    "        relative_path = os.path.relpath(audio_file, input_folder)\n",
    "        process_audio_file(audio_file, output_folder, relative_path, pseudonym_dict)\n",
    "\n",
    "    logging.info(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperX-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
