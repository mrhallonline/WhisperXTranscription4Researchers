{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0.0 WhisperX/Pyannote Transcription+Diarization Pipeline \n",
    "\n",
    "This Jupyter notebook is designed to test and evaluate a new Transcription and Diarization Pipeline with the following objectives:\n",
    "1. Achieving word-level transcription accuracy to ensure detailed and precise text representation of the audio input.\n",
    "2. Assessing diarization confidence levels to accurately attribute spoken segments to different speakers and measure the reliability of speaker identification.\n",
    "3. Enhancing the alignment of transcriptions to be closer to natural sentence segments, thereby improving the readability and usability of the transcribed data.\n",
    "\n",
    "The notebook leverages advanced transcription and diarization capabilities provided by the Whisper, WhisperX, and pyannote libraries. By using GPU acceleration, it processes audio data efficiently, performing alignment and diarization to produce structured outputs that are saved in CSV format for further analysis. The resources and installation instructions are included to facilitate the setup and execution of the pipeline.\n",
    "\n",
    "Resources:\n",
    "https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Setup\n",
    "WhisperX documentation found here: https://github.com/m-bain/whisperX\n",
    "\n",
    "1. Create Python environment\n",
    "conda create -n whisperx-env python=3.9\n",
    "conda activate whisperx-env\n",
    "\n",
    "2. Install PyTorch https://pytorch.org/get-started/locally/ \n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "3. Install WhisperX repository\n",
    "pip install git+https://github.com/m-bain/whisperx.git\n",
    "\n",
    "4. Additional useful packages to install\n",
    "pip install charset-normalizer\n",
    "pip install pandas\n",
    "pip install nltk\n",
    "pip install numpy\n",
    "pip install plotly\n",
    "pip install matplotlib\n",
    "pip install jupyter ipywidgets\n",
    "pip install webvtt-py\n",
    "pip install pypi-json\n",
    "pip install srt\n",
    "\n",
    "5. Create .env file at the same level as this notebook file with the following line\n",
    "HF_TOKEN=\"REPLACEWITHHUGGINGFACETOKENHERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Check once to see if CUDA GPU is available and PyTorch is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Laptop GPU')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA GPU is available to PyTorch\n",
    "import torch                                                # PyTorch\n",
    "torch.cuda.set_device(0)                                    # Set the main GPU as device to use if present\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()      # Check if GPU is available and get the name of the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Start here by adjusting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Laptop GPU')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Set the device and other configuration variables\n",
    "# Import necessary libraries\n",
    "import os                                                   # OS\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd                                         # Pandas\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchaudio._backend\") # Ignore torchaudio warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,message=\".*set_audio_backend has been deprecated.*\") # Ignore torchaudio warnings\n",
    "\n",
    "import torch                                                # PyTorch\n",
    "import whisperx                                             # Import the whisperx library    \n",
    "import gc                                                   # for garbage collection   \n",
    "import datetime                                             # for timing the process\n",
    "from whisperx.utils import get_writer                       # Import the get_writer function from the whisperx library to write the transcripts to a file\n",
    "import json                                                 # Import the json library to convert the JSON string to a JSON object\n",
    "import webvtt                                               # Import the webvtt library to convert the VTT file to a JSON string\n",
    "import srt                                                  # Import the srt library to convert the SRT file to a JSON string\n",
    "import warnings\n",
    "\n",
    "# Load the environment variables\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')                            # You can just replace this with your Hugging Face API token if you don't want to use the .env file\n",
    "\n",
    "# Check if GPU is available\n",
    "torch.cuda.set_device(0)                                    # Change to 0, 1, 2, 3, 4, 5, 6, 7 depending on which GPU you want to use\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     # Set the device\n",
    "\n",
    "batch_size = 16                                             # change to 4 if low on GPU memory (may reduce accuracy) highest is 32\n",
    "compute_type = \"float16\"                                    # change to \"int8\" if low on GPU memory (may reduce accuracy) highest is \"float32\" others are \"float16\" and \"int8\"\n",
    "hf_token = HF_TOKEN                                         # Replace your Hugging Face API token in the .env file\n",
    "whisperx_model = \"small.en\"                                 # change to \"large-v2\" for a larger model, others  are \"small.en\", \"medium.en\", or \"large.en\"\n",
    "\n",
    "# Paths\n",
    "base_dir = 'Data/rawAudioFiles'                             # Replace with the path to your main folder containing subfolders with audio files\n",
    "output_base_dir = 'Data/rawTranscriptFiles'                 # Replace with the path to the folder where you want to save the transcripts\n",
    "file_type1 = '.ogg'                                         # Change to 'mp3' if your audio files are in mp3 format\n",
    "file_type2 = '.m4a'                                         # Change to 'WAV' if your audio files are in WAV format\n",
    "file_type3 = '.mp3'                                         # Change to 'wav' if your audio files are in wav format (case dependent)\n",
    "\n",
    "# Define the file extensions to look for\n",
    "extensions = [file_type1, file_type2, file_type3]\n",
    "\n",
    "# Load pseudonyms CSV for anonymizing the transcripts\n",
    "pseudonyms_df = pd.read_csv('data/pseudonyms.csv')                              # Load the pseudonyms CSV file. in the format name,pseudonym as column headers\n",
    "pseudonym_dict = dict(zip(pseudonyms_df['name'], pseudonyms_df['pseudonym']))   # Create a pseudonym dictionary from the CSV file, only stored in ram\n",
    "\n",
    "\n",
    "torch.cuda.is_available(),torch.cuda.get_device_name()      # Check if GPU is available and show the name of the card, this is just here to help with a last second debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Run after adjusting variables first\n",
    "\n",
    "Just push run here. You shouldn't need to change anything here unless you want to output less or more file types. These are mostly functions which are then called at the end of the cell.\n",
    "You should see an output similar to the following:\n",
    "\n",
    "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
    "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
    "Data has been written to Data/Trancripts_Outputs in CSV, TXT, JSON, and VTT formats\n",
    "1 Data/RawAudioFiles_Inputs\\Monologue.ogg has been processed and saved in Data/Trancripts_Outputs\n",
    "1 audio files have been processed and saved in Data/Trancripts_Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found audio file: Data/rawAudioFiles\\Monologue1.ogg\n",
      "Found audio file: Data/rawAudioFiles\\Monologue2.ogg\n",
      "Found 2 audio files.\n",
      "Processing file: Data/rawAudioFiles\\Monologue1.ogg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\mrhal\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Data has been written to Data/rawTranscripts for Monologue1 in the following formats: CSV, TXT, JSON, and VTT\n",
      "1 Data/rawAudioFiles\\Monologue1.ogg has been processed and saved\n",
      "Processing file: Data/rawAudioFiles\\Monologue2.ogg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\mrhal\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Data has been written to Data/rawTranscripts for Monologue2 in the following formats: CSV, TXT, JSON, and VTT\n",
      "2 Data/rawAudioFiles\\Monologue2.ogg has been processed and saved\n",
      "2 audio files have been processed and saved in Data/rawTranscripts\n"
     ]
    }
   ],
   "source": [
    "# 2. Functions for transcribing and diarization of audio files\n",
    "# A. Function to find audio files in subfolders\n",
    "def find_audio_files(base_dir, extensions): \n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for audio_file in files:\n",
    "            if any(audio_file.endswith(ext) for ext in extensions):\n",
    "                full_audio_path = os.path.join(root, audio_file)\n",
    "                audio_files.append(full_audio_path)\n",
    "                print(f\"Found audio file: {full_audio_path}\")\n",
    "    return audio_files\n",
    "\n",
    "# B. Function to get file modification date\n",
    "def get_file_modification_date(file_path):\n",
    "    timestamp = os.path.getmtime(file_path)\n",
    "    date = datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
    "    return date\n",
    "\n",
    "# Function to anonymize text\n",
    "def anonymize_text(text, pseudonym_dict):\n",
    "    for real_name, pseudonym in pseudonym_dict.items():\n",
    "        text = text.replace(real_name, pseudonym)\n",
    "    return text\n",
    "\n",
    "# Function to convert segments to different formats and save\n",
    "def save_transcripts(segments, output_dir, filename):\n",
    "    # Anonymize segments\n",
    "    for segment in segments:\n",
    "        segment['text'] = anonymize_text(segment['text'], pseudonym_dict)\n",
    "    \n",
    "    # Add sentence numbers\n",
    "    for i, segment in enumerate(segments):\n",
    "        segment['sentence_number'] = i + 1\n",
    "    \n",
    "    # Convert segments to DataFrame and reorder columns\n",
    "    df = pd.DataFrame(segments)\n",
    "\n",
    "    # Clean leading spaces from the 'text' column\n",
    "    df['text'] = df['text'].apply(lambda x: x.lstrip())\n",
    "\n",
    "    # Reorder columns, ensuring 'sentence_number' is first\n",
    "    cols = df.columns.tolist()\n",
    "    cols = ['sentence_number'] + [col for col in cols if col != 'sentence_number']\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_output_file = os.path.join(output_dir, f'{filename}_transcription.csv')\n",
    "    df.to_csv(csv_output_file, index=False)\n",
    "    \n",
    "    # Save as TXT\n",
    "    txt_output_file = os.path.join(output_dir, f'{filename}_transcription.txt')\n",
    "    with open(txt_output_file, 'w', encoding='utf-8') as f:\n",
    "        for segment in segments:\n",
    "            # Strip leading spaces from the text\n",
    "            clean_text = segment['text'].rstrip().lstrip()\n",
    "            f.write(f\"{clean_text}\\n\")\n",
    "\n",
    "    # Save as JSON \n",
    "    json_output_file = os.path.join(output_dir, f'{filename}_transcription.json')\n",
    "    # Reorder segments and clean the text field\n",
    "    segments_reordered = [{k: segment[k].lstrip() if k == 'text' else segment[k] for k in cols} for segment in segments]\n",
    "    with open(json_output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(segments_reordered, f, ensure_ascii=False, indent=4)\n",
    "  \n",
    "    # Save as VTT\n",
    "    vtt_output_file = os.path.join(output_dir, f'{filename}_transcription.vtt')\n",
    "    vtt = webvtt.WebVTT()\n",
    "    for segment in segments:\n",
    "        vtt_segment = webvtt.Caption()\n",
    "        vtt_segment.start = str(datetime.timedelta(seconds=segment['start']))\n",
    "        vtt_segment.end = str(datetime.timedelta(seconds=segment['end']))\n",
    "        # Clean leading spaces from the text and format it with the sentence number\n",
    "        clean_text = segment['text'].lstrip().rstrip()\n",
    "        vtt_segment.lines = [f\"{segment['sentence_number']}: {clean_text}\"]\n",
    "        vtt.captions.append(vtt_segment)\n",
    "    vtt.save(vtt_output_file)\n",
    "\n",
    "\n",
    "    print(f\"Data has been written to {output_dir} for {filename} in the following formats: CSV, TXT, JSON, and VTT\")\n",
    "\n",
    "# C. Function to process each audio file\n",
    "def process_audio_file(audio_file, output_dir):\n",
    "    try:\n",
    "        print(f\"Processing file: {audio_file}\")\n",
    "        # Load audio\n",
    "        audio = whisperx.load_audio(audio_file)\n",
    "        \n",
    "        # Load and transcribe using WhisperX model\n",
    "        model = whisperx.load_model(whisperx_model, device, compute_type=compute_type)\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "        #print(result[\"segments\"])                          # you can uncomment this line to see the transcription results\n",
    "\n",
    "        # Clean up model from GPU if needed\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Align WhisperX output\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        #print(result[\"segments\"])                          # you can uncomment this line to see the alignment results\n",
    "\n",
    "        # Clean up alignment model from GPU if needed\n",
    "        del model_a\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Diarization with WhisperX\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "        diarize_segments = diarize_model(audio)\n",
    "        result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "        # print(diarize_segments)                           # you can uncomment this line to see the diarization results\n",
    "        # print(result[\"segments\"])                         # you can uncomment this line to see the diarization results\n",
    "\n",
    "        \n",
    "        # Save transcripts in multiple formats\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "        save_transcripts(result[\"segments\"], output_dir, filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {audio_file}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Ensure that all models are cleaned from memory\n",
    "        del diarize_model                                   # Clean up diarize_model\n",
    "        del result                                          # Clean up result\n",
    "        gc.collect()                                        # Garbage collection\n",
    "        torch.cuda.empty_cache()                            # Empty cache\n",
    "\n",
    "# D. Main function to execute the tasks\n",
    "def main(base_dir, output_base_dir, extensions):\n",
    "    audio_files = find_audio_files(base_dir, extensions)\n",
    "    print(f\"Found {len(audio_files)} audio files.\")\n",
    "    counter = 1\n",
    "    output_dir = output_base_dir  # Initialize output_dir to the base output directory at the start\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        process_audio_file(audio_file, output_dir)\n",
    "        print(f\"{counter} {audio_file} has been processed and saved\")\n",
    "        counter += 1\n",
    "\n",
    "    if counter > 1:\n",
    "        print(f\"{counter - 1} audio files have been processed and saved in {output_dir}\")\n",
    "    else:\n",
    "        print(\"No audio files were processed.\")\n",
    "\n",
    "# E. Execute the main function\n",
    "main(base_dir, output_base_dir, extensions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-diarize-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
